{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import re\n",
    "import os\n",
    "import urllib \n",
    "import nltk\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#https://stackoverflow.com/questions/37513355/converting-pandas-dataframe-into-spark-dataframe-error\n",
    "\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Getting the current directory of interest\n",
    "thisdir = '/project/ds5559/twitter_sentiment_analysis_group/'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.executor.memory\", '20g') \\\n",
    "        .config('spark.executor.cores', '8') \\\n",
    "        .config('spark.executor.instances', '2') \\\n",
    "        .config(\"spark.driver.memory\",'1g') \\\n",
    "        .appName(\"twitter_project\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "        # use all cores on local machine\n",
    "    # will see appName on cluster manager\n",
    "    # RAM per executor (worker)\n",
    "    # cores available to EACH executor\n",
    "#      total number of executors\n",
    "# RAM for driver, generally lower need than a worker\n",
    "\n",
    "#         .config(\"spark.driver.extraClassPath\", \"lib/sparknlp.jar\") \\\n",
    "#         .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.0\") \\\n",
    "    \n",
    "# df = spark.read.csv(thisdir)\n",
    "\n",
    "# df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210318_0309.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210322_0254.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210320_1148.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210321_1119.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210318_1359.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210322_1518.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_early_attempt.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210319_1252.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210319_2253.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/testing_deleteme.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210320_0902.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (0,4,6,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210323_0513.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210317_1203.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/tmp_pandas_df.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210329_0858.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210321_0531.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210321_0055.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210321_1044.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210327_0836.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210318_1401.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210320_4511.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210322_2104.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210323_1639.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210324_0514.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Getting the current directory of interest\n",
    "thisdir = '/project/ds5559/twitter_sentiment_analysis_group/'\n",
    "\n",
    "def pd_df_from_csvs_in_directory(directory):\n",
    "    \"\"\"\n",
    "    Accept directory, convert each CSV to pandas dataframe\n",
    "    then join all dataframes to single dataframe output.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    # r=root, d=directories, f = files\n",
    "    for r, d, f in os.walk(directory):\n",
    "        for file in f:\n",
    "            if file.endswith(\".csv\"):\n",
    "                filedir = os.path.join(r, file)\n",
    "                print(filedir)\n",
    "                try:\n",
    "                    df = pd.concat([df, pd.read_csv(filedir, index_col = 0)])\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "    return df\n",
    "\n",
    "dfALL = pd_df_from_csvs_in_directory(thisdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2119921"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfALL.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 ¬µs, sys: 0 ns, total: 2 ¬µs\n",
      "Wall time: 6.91 ¬µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "dfALL = dfALL.replace(r\"[\\n|\\t|\\r]\", \" \", regex=True)\n",
    "dfALL.to_csv(thisdir + \"tmp_pandas_df.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2119921"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.option(\"delimiter\", \"\\t\").option(\"header\", \"true\").csv(thisdir + \"tmp_pandas_df.csv\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='0', scraped_hashtag='#vaccine', scraped_order='0.0', created_at='Thu Mar 18 01:05:26 +0000 2021', id_str='1.3723534175075656e+18', text='BIG NEWS from @GovSisolak: #COVID19Vaccine eligibility expands to all Nevadans aged 16+ w/ underlying conditions on‚Ä¶ https://t.co/VNaabOLERs', truncated='True', in_reply_to_screen_name=None, retweet_count='0.0', favorite_count='0.0', lang='en', screen_name='atdleft', user_name='Andrew Davey', user_description=\"Muckraker, troublemaker, & no-BS-taker. I'm a Nevada based writer who's done a lot, but isn't done with it just yet.\", user_verified='False', user_followers_count='2034.0', hashtags=\"['COVID19Vaccine']\", symbols='[]', og_tweet_by=None, og_tweet_truncated=None),\n",
       " Row(_c0='1', scraped_hashtag='#vaccine', scraped_order='0.0', created_at='Thu Mar 18 01:05:15 +0000 2021', id_str='1.372353373123408e+18', text='@TerryBrady2097 This #Health #COVID19 #Vaccine rollout is sadly looking to be a repeat of the #agedcare pathway man‚Ä¶ https://t.co/RpoLBdwOXp', truncated='True', in_reply_to_screen_name='TerryBrady2097', retweet_count='0.0', favorite_count='0.0', lang='en', screen_name='pully8', user_name='rose lane', user_description=None, user_verified='False', user_followers_count='513.0', hashtags=\"['Health', 'COVID19', 'Vaccine', 'agedcare']\", symbols='[]', og_tweet_by=None, og_tweet_truncated=None),\n",
       " Row(_c0='2', scraped_hashtag='#vaccine', scraped_order='0.0', created_at='Thu Mar 18 01:04:30 +0000 2021', id_str='1.37235318146057e+18', text='RT @GeoRebekah: Attention #Florida!  If you, a family member or a friend are unable to get to the #COVID19 vaccination sites in-person, you‚Ä¶', truncated='False', in_reply_to_screen_name=None, retweet_count='348.0', favorite_count='0.0', lang='en', screen_name='hawkriver', user_name='Left Hand of Snarkness üò∑', user_description='Just dropped in to see what condition my condition was in. #Resistance #Liberal #BLM #IAmAnAlly', user_verified='False', user_followers_count='1927.0', hashtags=\"['Florida', 'COVID19']\", symbols='[]', og_tweet_by='GeoRebekah; Rebekah Jones', og_tweet_truncated='True'),\n",
       " Row(_c0='3', scraped_hashtag='#vaccine', scraped_order='0.0', created_at='Thu Mar 18 01:04:29 +0000 2021', id_str='1.3723531770565386e+18', text='RT @JulianHillMP: Scotty promised Australians we‚Äôd be ‚Äúat the front of the vaccine queue‚Äù globally.  The PM made sure he was first in line.‚Ä¶', truncated='False', in_reply_to_screen_name=None, retweet_count='499.0', favorite_count='0.0', lang='en', screen_name='pennyfortheguy', user_name='Penny', user_description='Unconventional lover of the unconventional', user_verified='False', user_followers_count='135.0', hashtags='[]', symbols='[]', og_tweet_by='JulianHillMP; Julian Hill MP', og_tweet_truncated='True')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sparknlp in /home/cam7cu/.local/lib/python3.7/site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sparknlp) (1.18.1)\n",
      "Requirement already satisfied: spark-nlp in /home/cam7cu/.local/lib/python3.7/site-packages (from sparknlp) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "#https://towardsdatascience.com/natural-language-processing-with-pyspark-and-spark-nlp-b5b29f8faba\n",
    "\n",
    "!pip install sparknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://udc-ba27-18:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>twitter_project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe96f41dc90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparknlp\n",
    "sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_242\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08)\n",
      "OpenJDK 64-Bit Server VM (build 25.242-b08, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.5\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.11.12, OpenJDK 64-Bit Server VM, 1.8.0_242\n",
      "Branch HEAD\n",
      "Compiled by user centos on 2020-02-02T19:38:06Z\n",
      "Revision cee4ecbb16917fa85f02c635925e2687400aa56b\n",
      "Url https://gitbox.apache.org/repos/asf/spark.git\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sparknlp.base import Finisher, DocumentAssembler\n",
    "# from pyspark.ml import Pipeline\n",
    "# from sparknlp.annotator import Tokenizer, Normalizer, LemmatizerModel, StopWordsCleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documentAssembler = DocumentAssembler() \\\n",
    "#      .setInputCol('text') \\\n",
    "#      .setOutputCol('document')\n",
    "\n",
    "# tokenizer = Tokenizer() \\\n",
    "#      .setInputCols(['document']) \\\n",
    "#      .setOutputCol('token')\n",
    "\n",
    "# # note normalizer defaults to changing all words to lowercase.\n",
    "# # Use .setLowercase(False) to maintain input case.\"\"\n",
    "# normalizer = Normalizer() \\\n",
    "#      .setInputCols(['token']) \\\n",
    "#      .setOutputCol('normalized') \\\n",
    "#      .setLowercase(True)\n",
    "\n",
    "# # note that lemmatizer needs a dictionary. So I used the pre-trained\n",
    "# # model (note that it defaults to english)\n",
    "# lemmatizer = LemmatizerModel.pretrained() \\\n",
    "#      .setInputCols(['normalized']) \\\n",
    "#      .setOutputCol('lemma')\n",
    "\n",
    "# stopwords_cleaner = StopWordsCleaner() \\\n",
    "#      .setInputCols(['lemma']) \\\n",
    "#      .setOutputCol('clean_lemma') \\\n",
    "#      .setCaseSensitive(False) \\\n",
    "#      .setStopWords(eng_stopwords)\n",
    "\n",
    "# # finisher converts tokens to human-readable output\n",
    "# finisher = Finisher() \\\n",
    "#      .setInputCols(['clean_lemma']) \\\n",
    "#      .setCleanAnnotations(False)\n",
    "\n",
    "# pipeline = Pipeline() \\\n",
    "#      .setStages([\n",
    "#            documentAssembler,\n",
    "#            tokenizer,\n",
    "#            normalizer,\n",
    "#            lemmatizer,\n",
    "#            stopwords_cleaner,\n",
    "#            finisher\n",
    "#      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # transform text with the pipeline\n",
    "# equifax = pipeline.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/cam7cu/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdata = df.na.drop(subset=[\"text\"]).drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+--------+\n",
      "|  neg|  neu|  pos|compound|\n",
      "+-----+-----+-----+--------+\n",
      "|  0.0| 0.92| 0.08|  0.1027|\n",
      "|0.141|0.859|  0.0| -0.4215|\n",
      "|  0.0|0.851|0.149|  0.5411|\n",
      "|  0.0|0.821|0.179|  0.5859|\n",
      "|  0.0|0.816|0.184|  0.6467|\n",
      "+-----+-----+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = dfdata.rdd\\\n",
    "    .map(lambda x: list(sid.polarity_scores(x['text']).values()))\\\n",
    "    .toDF([\"neg\", \"neu\", \"pos\", \"compound\"])\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+-------------+--------------------+--------------------+--------------------+---------+-----------------------+-------------+--------------+----+---------------+--------------------+--------------------+-------------+--------------------+--------------------+-------+--------------------+------------------+------+-----+-----+-----+--------+------+\n",
      "| _c0|scraped_hashtag|scraped_order|          created_at|              id_str|                text|truncated|in_reply_to_screen_name|retweet_count|favorite_count|lang|    screen_name|           user_name|    user_description|user_verified|user_followers_count|            hashtags|symbols|         og_tweet_by|og_tweet_truncated|row_id|  neg|  neu|  pos|compound|row_id|\n",
      "+----+---------------+-------------+--------------------+--------------------+--------------------+---------+-----------------------+-------------+--------------+----+---------------+--------------------+--------------------+-------------+--------------------+--------------------+-------+--------------------+------------------+------+-----+-----+-----+--------+------+\n",
      "|  26|       #vaccine|          0.0|Thu Mar 18 01:00:...|1.372352049828991...|More than 5,000 C...|     True|                   null|          1.0|           3.0|  en|CE_ChinaEconomy|       China Economy|Latest business n...|         True|              2171.0|['Egypt', 'Sinoph...|     []|                null|              null|    26|  0.0|  1.0|  0.0|     0.0|    26|\n",
      "|  29|       #vaccine|          0.0|Thu Mar 18 00:58:...|1.372351792131014...|RT @mi6rogue: Vac...|    False|                   null|         63.0|           0.0|  en|   qick4267_tom|Tom Jaques (#FBPE...|                null|        False|              2115.0|                  []|     []| mi6rogue; MI6 ROGUE|              True|    29| 0.11| 0.89|  0.0| -0.4767|    29|\n",
      "| 474|       #vaccine|          0.0|Wed Mar 17 23:22:...|1.372327454040277...|RT @MimiJ9: Asked...|    False|                   null|          3.0|           0.0|  en|     jfranklynh|James Harris #FBP...|#FBPE #REJOINEU #...|        False|              4626.0|         ['vaccine']|     []|       MimiJ9; MimiJ|             False|   474|  0.0|  1.0|  0.0|     0.0|   474|\n",
      "| 964|       #vaccine|          0.0|Wed Mar 17 22:11:...|1.372309542739046...|The EU is threate...|     True|                   null|          0.0|           1.0|  en|  FinancialNews|      Financial News|FN is the leading...|         True|             56774.0|                  []|     []|                null|              null|   964|0.368|0.632|  0.0| -0.8519|   964|\n",
      "|1677|       #vaccine|          0.0|Wed Mar 17 20:44:...|1.372287763874189...|RT @GIANTmicrobes...|    False|                   null|          3.0|           0.0|  en|       PepoteCC|           Red_Viper|Estudiante de Bio...|        False|               219.0|                  []|     []|GIANTmicrobes; GI...|              True|  1677|  0.0|0.805|0.195|  0.6239|  1677|\n",
      "+----+---------------+-------------+--------------------+--------------------+--------------------+---------+-----------------------+-------------+--------------+----+---------------+--------------------+--------------------+-------------+--------------------+--------------------+-------+--------------------+------------------+------+-----+-----+-----+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "ddf1 = dfdata.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "ddf2 = df2.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "result = ddf1.join(ddf2, ddf1.row_id == ddf2.row_id) #.drop(\"row_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------+--------------------+--------------------+--------------------+---------+-----------------------+-------------+--------------+----+---------------+-------------+--------------------+-------------+--------------------+--------------------+-------+-----------+------------------+------+---+---+---+--------+------+\n",
      "|_c0|scraped_hashtag|scraped_order|          created_at|              id_str|                text|truncated|in_reply_to_screen_name|retweet_count|favorite_count|lang|    screen_name|    user_name|    user_description|user_verified|user_followers_count|            hashtags|symbols|og_tweet_by|og_tweet_truncated|row_id|neg|neu|pos|compound|row_id|\n",
      "+---+---------------+-------------+--------------------+--------------------+--------------------+---------+-----------------------+-------------+--------------+----+---------------+-------------+--------------------+-------------+--------------------+--------------------+-------+-----------+------------------+------+---+---+---+--------+------+\n",
      "| 26|       #vaccine|          0.0|Thu Mar 18 01:00:...|1.372352049828991...|More than 5,000 C...|     True|                   null|          1.0|           3.0|  en|CE_ChinaEconomy|China Economy|Latest business n...|         True|              2171.0|['Egypt', 'Sinoph...|     []|       null|              null|    26|0.0|1.0|0.0|     0.0|    26|\n",
      "+---+---------------+-------------+--------------------+--------------------+--------------------+---------+-----------------------+-------------+--------------+----+---------------+-------------+--------------------+-------------+--------------------+--------------------+-------+-----------+------------------+------+---+---+---+--------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "# label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf]) #, label_stringIdx])\n",
    "\n",
    "pipelineFit = pipeline.fit(result)\n",
    "train_df = pipelineFit.transform(result)\n",
    "# val_df = pipelineFit.transform(val_set)\n",
    "# train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------+--------------------+--------------------+--------------------+---------+-----------------------+-------------+--------------+----+---------------+-------------+--------------------+-------------+--------------------+--------------------+-------+-----------+------------------+------+---+---+---+--------+------+--------------------+--------------------+--------------------+\n",
      "|_c0|scraped_hashtag|scraped_order|          created_at|              id_str|                text|truncated|in_reply_to_screen_name|retweet_count|favorite_count|lang|    screen_name|    user_name|    user_description|user_verified|user_followers_count|            hashtags|symbols|og_tweet_by|og_tweet_truncated|row_id|neg|neu|pos|compound|row_id|               words|                  tf|            features|\n",
      "+---+---------------+-------------+--------------------+--------------------+--------------------+---------+-----------------------+-------------+--------------+----+---------------+-------------+--------------------+-------------+--------------------+--------------------+-------+-----------+------------------+------+---+---+---+--------+------+--------------------+--------------------+--------------------+\n",
      "| 26|       #vaccine|          0.0|Thu Mar 18 01:00:...|1.372352049828991...|More than 5,000 C...|     True|                   null|          1.0|           3.0|  en|CE_ChinaEconomy|China Economy|Latest business n...|         True|              2171.0|['Egypt', 'Sinoph...|     []|       null|              null|    26|0.0|1.0|0.0|     0.0|    26|[more, than, 5,00...|(65536,[1455,1315...|(65536,[1455,1315...|\n",
      "+---+---------------+-------------+--------------------+--------------------+--------------------+---------+-----------------------+-------------+--------------+----+---------------+-------------+--------------------+-------------+--------------------+--------------------+-------+-----------+------------------+------+---+---+---+--------+------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "col should be Column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-c87343531c28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Add a dummy column to groupBy & in a single line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dummy_col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"array_col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#Collect_set will return you an array without duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m         \"\"\"\n\u001b[0;32m-> 1996\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1997\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: col should be Column"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import  ArrayType, StringType\n",
    "import re\n",
    "    \n",
    "#Add a dummy column to groupBy & in a single line\n",
    "dfg = train_df.withColumn(\"dummy_col\", 1)\n",
    "dfg = train_df.withColumn(\"array_col\", F.split(\"text\", \" \"))\n",
    "#Collect_set will return you an array without duplicates\n",
    "df_grp = dfg.groupBy(\"dummy_col\").agg(F.collect_set(\"array_col\").alias(\"array_col\"))\n",
    "#explode to transpoe the column\n",
    "df_grp = df_grp.withColumn(\"explode_col\", F.explode(\"array_col\"))\n",
    "df_grp = df_grp.withColumn(\"explode_col\", F.explode(\"explode_col\"))\n",
    "#Distince to remove the duplicates\n",
    "df_grp = df_grp.select(\"explode_col\").distinct()\n",
    "#another dummy column to create the row number\n",
    "df_grp = df_grp.withColumn(\"dummy_col\", F.lit(\"A\"))\n",
    "_w = W.partitionBy(\"dummy_col\").orderBy(\"dummy_col\")\n",
    "df_grp = df_grp.withColumn(\"rnk\", F.row_number().over(_w))\n",
    "df_grp.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def equivalent_type(f):\n",
    "#     \"\"\"\n",
    "#     Read Pandas datatype and convert to PySpark datatype. \n",
    "#     \"\"\"\n",
    "#     if f == 'datetime64[ns]': return TimestampType()\n",
    "#     elif f == 'int64': return LongType()\n",
    "#     elif f == 'int32': return IntegerType()\n",
    "#     elif f == 'float64': return FloatType()\n",
    "#     else: return StringType()\n",
    "\n",
    "# def define_structure(string, format_type):\n",
    "#     \"\"\"\n",
    "#     Automatically define the structure of \n",
    "#     each column from pandas dataframe.\n",
    "#     Returns PySpark StructFeild object. \n",
    "#     \"\"\"\n",
    "#     try: typo = equivalent_type(format_type)\n",
    "#     except: typo = StringType()\n",
    "#     return StructField(string, typo)\n",
    "\n",
    "# # Given pandas dataframe, it will return a spark's dataframe.\n",
    "# def pandas_to_spark(pandas_df):\n",
    "#     \"\"\"\n",
    "#     Accepts pandas dataframe and converts to\n",
    "#     PySpark dataframe. \n",
    "#     \"\"\"\n",
    "#     columns = list(pandas_df.columns)\n",
    "#     types = list(pandas_df.dtypes)\n",
    "#     struct_list = []\n",
    "#     for column, typo in zip(columns, types): \n",
    "#       struct_list.append(define_structure(column, typo))\n",
    "#     p_schema = StructType(struct_list)\n",
    "#     return sqlContext.createDataFrame(pandas_df, p_schema)\n",
    "\n",
    "\n",
    "sparkdf = pandas_to_spark(dfALL)\n",
    "sparkdf.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfALL.index)\n",
    "\n",
    "spark.read.option(\"delimiter\", \"\\t\").csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = sparkdf.repartition(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkdf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dfALL[[\"scraped_hashtag\", \"text\", \"user_description\"]].drop_duplicates()\n",
    "data.index.names = [\"doc_id\"]\n",
    "data = data.set_index(['scraped_hashtag'], append=True)\n",
    "data['fulltext'] = data.text + \" \" + data.user_description.fillna(\"\")\n",
    "data = data.dropna().copy()\n",
    "data = data.drop(columns=[\"text\", \"user_description\"])\n",
    "data\n",
    "#477,640 unique records by doc_id, scraped_hashtag, and fulltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# tknzr = TweetTokenizer()\n",
    "\n",
    "# def word_tokenize(x):\n",
    "#     r = pd.Series(tknzr.tokenize(x))\n",
    "#     #pd.Series(nltk.pos_tag(tknzr.tokenize(x)))\n",
    "#     return r\n",
    "\n",
    "# OHCO = [\"doc_id\", \"scraped_hashtag\", \"token_num\"]\n",
    "# df = data.fulltext.apply(lambda x: pd.Series(tknzr.tokenize(x))).stack().to_frame().rename(columns={0:'term_str'})\n",
    "# # df.index.names = OHCO\n",
    "\n",
    "# # df['pos'] = df.pos_tuple.apply(lambda x: x[1])\n",
    "# # df['token_str'] = df.pos_tuple.apply(lambda x: x[0])\n",
    "# # df = df.drop('pos_tuple', 1)\n",
    "# # df['term_str'] = df['token_str'].str.lower().str.replace(r'[\\W_]', '')\n",
    "# # #ADD STEMS\n",
    "# # stemmer = nltk.stem.SnowballStemmer('english')\n",
    "# # df['p_stem'] = df.term_str.apply(stemmer.stem)\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(data.fulltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = vectorizer.get_feature_names()\n",
    "print(len(feats))\n",
    "#Unique words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stops = list(nltk.corpus.stopwords.words('english'))\n",
    "nostops = [x for x in feats if x not in stops]\n",
    "len(nostops)\n",
    "#unique words in the dataset which are not stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer()\n",
    "\n",
    "rsh = data.reset_index()\n",
    "rsh.scraped_hashtag = rsh.scraped_hashtag + \" x\"\n",
    "searchtags = vectorizer2.fit_transform(rsh.scraped_hashtag)\n",
    "sfeats = vectorizer2.get_feature_names()\n",
    "sfeats = list(set([str(x).replace(\"#\", \"\") for x in sfeats]))\n",
    "len(sfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_on = \"p_stem\"\n",
    "voc = TOKEN[base_on].value_counts()\\\n",
    "    .to_frame()\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns={base_on:'n', 'index':base_on})\\\n",
    "    .sort_values(base_on)\n",
    "voc.index.name = 'term_id'\n",
    "\n",
    "#LABEL STOPWORDS\n",
    "stops = list(nltk.corpus.stopwords.words('english')) + [\"thing\", \"one\"]\n",
    "sw = pd.DataFrame(nltk.corpus.stopwords.words('english'), columns=[base_on])\n",
    "sw = sw.reset_index().set_index(base_on)\n",
    "sw.columns = ['dummy']\n",
    "sw.dummy = 1\n",
    "voc['stopword'] = voc[base_on].map(sw.dummy)\n",
    "voc['stopword'] = voc['stopword'].fillna(0).astype('int')\n",
    "\n",
    "#LABEL FILLER WORDS\n",
    "fill_words = pd.DataFrame([\"uh\", \"um\", \"uhm\", \"just\", \"like\", \\\n",
    "    \"yeah\", \"right\", \"okay\",  \\\n",
    "    \"alright\", \"so\", \"mhmm\", \"really\"], columns=[base_on])\n",
    "fill_words = fill_words.reset_index().set_index(base_on)\n",
    "fill_words.columns = ['dummy']\n",
    "fill_words.dummy = 1\n",
    "voc['fill_word'] = voc[base_on].map(fill_words.dummy)\n",
    "voc['fill_word'] = voc['fill_word'].fillna(0).astype('int')\n",
    "\n",
    "# # #ADD STEMS\n",
    "# # stemmer = nltk.stem.SnowballStemmer('english')\n",
    "# # voc['p_stem'] = voc.term_str.apply(stemmer.stem)\n",
    "\n",
    "# # #ADD LEMMATIZED\n",
    "# # wnl = nltk.stem.WordNetLemmatizer()\n",
    "# # voc['p_lemm'] = voc.term_str.apply(wnl.lemmatize)\n",
    "\n",
    "#ADD POS_Max\n",
    "poscount = TOKEN.reset_index()[[base_on, \"pos\", \"token_num\"]].groupby([base_on, \"pos\"]).count().reset_index()\n",
    "posmax = poscount.groupby(base_on).max().reset_index()\n",
    "posmax.columns = [base_on, \"pos_max\", \"pos_count\"]\n",
    "voc = voc[[base_on, \"n\", \"stopword\", \"fill_word\"]].groupby([\"term_id\", base_on]).mean().reset_index()\n",
    "voc = voc.merge(posmax[[base_on, \"pos_max\"]], how='left', on=base_on)\n",
    "# voc = voc.set_index([\"term_id\"])\n",
    "\n",
    "#FIND ENTROPY\n",
    "n_tokens = voc.n.sum()\n",
    "voc['p'] = voc['n'] / n_tokens\n",
    "voc['i'] = np.log2(1/voc['p'])\n",
    "n_terms = voc.shape[0]\n",
    "H = (voc.p * voc.i).sum()\n",
    "Hmax = np.log2(n_terms)\n",
    "R = 1 - (H/Hmax)\n",
    "print(\"Entropy of this vocab: \", R)\n",
    "\n",
    "voc = voc.set_index(base_on)\n",
    "voc.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = np.log10(dfALL.query(\"user_followers_count > 100\").user_followers_count).hist(bins=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = np.log10(dfALL.query(\"user_followers_count > 100\").user_followers_count.drop_duplicates()).hist(bins=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " dfALL[[\"user_followers_count\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5559",
   "language": "python",
   "name": "ds5559"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
