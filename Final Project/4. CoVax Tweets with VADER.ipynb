{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. CoVax Tweets with VADER\n",
    "\n",
    "- Arshiya Ansari - aa9yk\n",
    "- Congxin (David) Xu - cx2rx\n",
    "- Pengwei (Tiger) Hu - ph3bz\n",
    "- Kip McCharen - cam7cu\n",
    "\n",
    "## Description\n",
    "\n",
    "This notebook is written to combine multiple scrapes of tweets from the Twitter API and analyze them using Valence Aware Dictionary and sEntiment Reasoner (VADER). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Modules, Start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import re\n",
    "import os\n",
    "import urllib \n",
    "import nltk\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the current directory of interest\n",
    "thisdir = '/project/ds5559/twitter_sentiment_analysis_group/'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.executor.memory\", '20g') \\\n",
    "        .config('spark.executor.cores', '8') \\\n",
    "        .config('spark.executor.instances', '2') \\\n",
    "        .config(\"spark.driver.memory\",'1g') \\\n",
    "        .appName(\"twitter_project\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210318_0309.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210322_0254.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210320_1148.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210321_1119.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210401_2123.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210404_0240.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210405_0241.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210318_1359.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210409_2329.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210322_1518.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_early_attempt.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210319_1252.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210319_2253.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/testing_deleteme.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210320_0902.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3337: DtypeWarning: Columns (0,4,6,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210323_0513.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210317_1203.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/tmp_pandas_df.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210329_0858.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210321_0531.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210321_0055.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210321_1044.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210327_0836.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210318_1401.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210320_4511.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210322_2104.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210407_0146.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210323_1639.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210324_0514.csv\n"
     ]
    }
   ],
   "source": [
    "def pd_df_from_csvs_in_directory(directory):\n",
    "    \"\"\"\n",
    "    Accept directory, convert each CSV to pandas dataframe\n",
    "    then join all dataframes to single dataframe output.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    # r=root, d=directories, f = files\n",
    "    for r, d, f in os.walk(directory):\n",
    "        for file in f:\n",
    "            if file.endswith(\".csv\"):\n",
    "                filedir = os.path.join(r, file)\n",
    "                print(filedir)\n",
    "                try:\n",
    "                    df = pd.concat([df, pd.read_csv(filedir, index_col = 0)])\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "    return df\n",
    "\n",
    "dfALL = pd_df_from_csvs_in_directory(thisdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3007437"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many records did we capture?\n",
    "len(dfALL.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: DF to Tab-Delimited to Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.72 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "##https://stackoverflow.com/questions/37513355/converting-pandas-dataframe-into-spark-dataframe-error\n",
    "dfALL = dfALL.replace(r\"[\\n|\\t|\\r]\", \" \", regex=True)\n",
    "dfALL.to_csv(thisdir + \"tmp_pandas_df.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"delimiter\", \"\\t\").option(\"header\", \"true\").csv(thisdir + \"tmp_pandas_df.csv\")\n",
    "df.count() #count records in Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------+--------------------+--------------------+--------------------+---------+-----------------------+-------------+--------------+----+--------------+--------------------+--------------------+-------------+--------------------+--------------------+-------+--------------------+------------------+\n",
      "|_c0|scraped_hashtag|scraped_order|          created_at|              id_str|                text|truncated|in_reply_to_screen_name|retweet_count|favorite_count|lang|   screen_name|           user_name|    user_description|user_verified|user_followers_count|            hashtags|symbols|         og_tweet_by|og_tweet_truncated|\n",
      "+---+---------------+-------------+--------------------+--------------------+--------------------+---------+-----------------------+-------------+--------------+----+--------------+--------------------+--------------------+-------------+--------------------+--------------------+-------+--------------------+------------------+\n",
      "|  0|       #vaccine|          0.0|Thu Mar 18 01:05:...|1.372353417507565...|BIG NEWS from @Go...|     True|                   null|          0.0|           0.0|  en|       atdleft|        Andrew Davey|Muckraker, troubl...|        False|              2034.0|  ['COVID19Vaccine']|     []|                null|              null|\n",
      "|  1|       #vaccine|          0.0|Thu Mar 18 01:05:...|1.372353373123408...|@TerryBrady2097 T...|     True|         TerryBrady2097|          0.0|           0.0|  en|        pully8|           rose lane|                null|        False|               513.0|['Health', 'COVID...|     []|                null|              null|\n",
      "|  2|       #vaccine|          0.0|Thu Mar 18 01:04:...|1.37235318146057e+18|RT @GeoRebekah: A...|    False|                   null|        348.0|           0.0|  en|     hawkriver|Left Hand of Snar...|Just dropped in t...|        False|              1927.0|['Florida', 'COVI...|     []|GeoRebekah; Rebek...|              True|\n",
      "|  3|       #vaccine|          0.0|Thu Mar 18 01:04:...|1.372353177056538...|RT @JulianHillMP:...|    False|                   null|        499.0|           0.0|  en|pennyfortheguy|               Penny|Unconventional lo...|        False|               135.0|                  []|     []|JulianHillMP; Jul...|              True|\n",
      "+---+---------------+-------------+--------------------+--------------------+--------------------+---------+-----------------------+-------------+--------------+----+--------------+--------------------+--------------------+-------------+--------------------+--------------------+-------+--------------------+------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a large dataset with many columns, we will have to narrow down the data to use it effectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Apply VADER Valence Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/cam7cu/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop NA rows in the text column, VADER returns errors if it encounters empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdata = df.na.drop(subset=[\"text\"]).drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+--------+\n",
      "|  neg|  neu|  pos|compound|\n",
      "+-----+-----+-----+--------+\n",
      "|  0.0| 0.92| 0.08|  0.1027|\n",
      "|0.141|0.859|  0.0| -0.4215|\n",
      "|  0.0|0.851|0.149|  0.5411|\n",
      "|  0.0|0.821|0.179|  0.5859|\n",
      "|  0.0|0.816|0.184|  0.6467|\n",
      "+-----+-----+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate polarity scores from VADER, export into its own dataframe\n",
    "df2 = dfdata.rdd\\\n",
    "    .map(lambda x: list(sid.polarity_scores(x['text']).values()))\\\n",
    "    .toDF([\"neg\", \"neu\", \"pos\", \"compound\"])\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's probably a much smarter way to do this, but I am adding a sequential ID and joining on that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "ddf1 = dfdata.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "ddf2 = df2.withColumn(\"row_id2\", monotonically_increasing_id())\n",
    "result = ddf1.join(ddf2, ddf1.row_id == ddf2.row_id2) #.drop(\"row_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------+--------------------+--------------------+--------------------+---------+-----------------------+-------------+--------------+----+---------------+-------------+--------------------+-------------+--------------------+--------------------+-------+-----------+------------------+------+---+---+---+--------+-------+\n",
      "|_c0|scraped_hashtag|scraped_order|          created_at|              id_str|                text|truncated|in_reply_to_screen_name|retweet_count|favorite_count|lang|    screen_name|    user_name|    user_description|user_verified|user_followers_count|            hashtags|symbols|og_tweet_by|og_tweet_truncated|row_id|neg|neu|pos|compound|row_id2|\n",
      "+---+---------------+-------------+--------------------+--------------------+--------------------+---------+-----------------------+-------------+--------------+----+---------------+-------------+--------------------+-------------+--------------------+--------------------+-------+-----------+------------------+------+---+---+---+--------+-------+\n",
      "| 26|       #vaccine|          0.0|Thu Mar 18 01:00:...|1.372352049828991...|More than 5,000 C...|     True|                   null|          1.0|           3.0|  en|CE_ChinaEconomy|China Economy|Latest business n...|         True|              2171.0|['Egypt', 'Sinoph...|     []|       null|              null|    26|0.0|1.0|0.0|     0.0|     26|\n",
      "+---+---------------+-------------+--------------------+--------------------+--------------------+---------+-----------------------+-------------+--------------+----+---------------+-------------+--------------------+-------------+--------------------+--------------------+-------+-----------+------------------+------+---+---+---+--------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Export to Directory of CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.write.csv('twitter_corpus_VADER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czf twitter_corpus_VADER.tar.gz twitter_corpus_VADER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
