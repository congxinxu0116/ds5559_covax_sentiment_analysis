{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. CoVax Tweets with VADER\n",
    "\n",
    "- Arshiya Ansari - aa9yk\n",
    "- Congxin (David) Xu - cx2rx\n",
    "- Pengwei (Tiger) Hu - ph3bz\n",
    "- Kip McCharen - cam7cu\n",
    "\n",
    "## Description\n",
    "\n",
    "This notebook is written to combine multiple scrapes of tweets from the Twitter API and analyze them using Valence Aware Dictionary and sEntiment Reasoner (VADER). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Modules, Start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import re\n",
    "import os\n",
    "import urllib \n",
    "import nltk\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the current directory of interest\n",
    "thisdir = '/project/ds5559/twitter_sentiment_analysis_group/'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.executor.memory\", '20g') \\\n",
    "        .config('spark.executor.cores', '8') \\\n",
    "        .config('spark.executor.instances', '2') \\\n",
    "        .config(\"spark.driver.memory\",'1g') \\\n",
    "        .appName(\"twitter_project\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210318_0309.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210322_0254.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210320_1148.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210321_1119.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210401_2123.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210404_0240.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210405_0241.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210318_1359.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210409_2329.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210322_1518.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_early_attempt.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210319_1252.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210319_2253.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/testing_deleteme.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210320_0902.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3337: DtypeWarning: Columns (0,4,6,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210323_0513.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210317_1203.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/tmp_pandas_df.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210329_0858.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210321_0531.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210321_0055.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210321_1044.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210327_0836.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210318_1401.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210320_4511.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210322_2104.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210407_0146.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210323_1639.csv\n",
      "/project/ds5559/twitter_sentiment_analysis_group/hashtag_output_210324_0514.csv\n"
     ]
    }
   ],
   "source": [
    "def pd_df_from_csvs_in_directory(directory):\n",
    "    \"\"\"\n",
    "    Accept directory, convert each CSV to pandas dataframe\n",
    "    then join all dataframes to single dataframe output.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    # r=root, d=directories, f = files\n",
    "    for r, d, f in os.walk(directory):\n",
    "        for file in f:\n",
    "            if file.endswith(\".csv\"):\n",
    "                filedir = os.path.join(r, file)\n",
    "                print(filedir)\n",
    "                try:\n",
    "                    df = pd.concat([df, pd.read_csv(filedir, index_col = 0)])\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "    return df\n",
    "\n",
    "dfALL = pd_df_from_csvs_in_directory(thisdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3007437"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many records did we capture?\n",
    "len(dfALL.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: DF to Tab-Delimited to Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.44 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "##https://stackoverflow.com/questions/37513355/converting-pandas-dataframe-into-spark-dataframe-error\n",
    "dfALL = dfALL.replace(r\"[\\n|\\t|\\r]\", \" \", regex=True)\n",
    "dfALL.to_csv(thisdir + \"tmp_pandas_df.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3007437"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.option(\"delimiter\", \"\\t\").option(\"header\", \"true\").csv(thisdir + \"tmp_pandas_df.csv\")\n",
    "df.count() #count records in Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------+--------------------+--------------------+--------------------+---------+-----------------------+-------------+--------------+----+--------------+--------------------+--------------------+-------------+--------------------+--------------------+-------+--------------------+------------------+\n",
      "|_c0|scraped_hashtag|scraped_order|          created_at|              id_str|                text|truncated|in_reply_to_screen_name|retweet_count|favorite_count|lang|   screen_name|           user_name|    user_description|user_verified|user_followers_count|            hashtags|symbols|         og_tweet_by|og_tweet_truncated|\n",
      "+---+---------------+-------------+--------------------+--------------------+--------------------+---------+-----------------------+-------------+--------------+----+--------------+--------------------+--------------------+-------------+--------------------+--------------------+-------+--------------------+------------------+\n",
      "|  0|       #vaccine|          0.0|Thu Mar 18 01:05:...|1.372353417507565...|BIG NEWS from @Go...|     True|                   null|          0.0|           0.0|  en|       atdleft|        Andrew Davey|Muckraker, troubl...|        False|              2034.0|  ['COVID19Vaccine']|     []|                null|              null|\n",
      "|  1|       #vaccine|          0.0|Thu Mar 18 01:05:...|1.372353373123408...|@TerryBrady2097 T...|     True|         TerryBrady2097|          0.0|           0.0|  en|        pully8|           rose lane|                null|        False|               513.0|['Health', 'COVID...|     []|                null|              null|\n",
      "|  2|       #vaccine|          0.0|Thu Mar 18 01:04:...|1.37235318146057e+18|RT @GeoRebekah: A...|    False|                   null|        348.0|           0.0|  en|     hawkriver|Left Hand of Snar...|Just dropped in t...|        False|              1927.0|['Florida', 'COVI...|     []|GeoRebekah; Rebek...|              True|\n",
      "|  3|       #vaccine|          0.0|Thu Mar 18 01:04:...|1.372353177056538...|RT @JulianHillMP:...|    False|                   null|        499.0|           0.0|  en|pennyfortheguy|               Penny|Unconventional lo...|        False|               135.0|                  []|     []|JulianHillMP; Jul...|              True|\n",
      "+---+---------------+-------------+--------------------+--------------------+--------------------+---------+-----------------------+-------------+--------------+----+--------------+--------------------+--------------------+-------------+--------------------+--------------------+-------+--------------------+------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a large dataset with many columns, we will have to narrow down the data to use it effectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Apply VADER Valence Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/cam7cu/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop NA rows in the text column, VADER returns errors if it encounters empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdata = df.na.drop(subset=[\"text\"]).drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+--------+\n",
      "|  neg|  neu|  pos|compound|\n",
      "+-----+-----+-----+--------+\n",
      "|  0.0| 0.92| 0.08|  0.1027|\n",
      "|0.141|0.859|  0.0| -0.4215|\n",
      "|  0.0|0.851|0.149|  0.5411|\n",
      "|  0.0|0.821|0.179|  0.5859|\n",
      "|  0.0|0.816|0.184|  0.6467|\n",
      "+-----+-----+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate polarity scores from VADER, export into its own dataframe\n",
    "df2 = dfdata.rdd\\\n",
    "    .map(lambda x: list(sid.polarity_scores(x['text']).values()))\\\n",
    "    .toDF([\"neg\", \"neu\", \"pos\", \"compound\"])\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's probably a much smarter way to do this, but I am adding a sequential ID and joining on that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "ddf1 = dfdata.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "ddf2 = df2.withColumn(\"row_id2\", monotonically_increasing_id())\n",
    "result = ddf1.join(ddf2, ddf1.row_id == ddf2.row_id2) #.drop(\"row_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------+--------------------+--------------------+--------------------+---------+-----------------------+-------------+--------------+----+---------------+-------------+--------------------+-------------+--------------------+--------------------+-------+-----------+------------------+------+---+---+---+--------+-------+\n",
      "|_c0|scraped_hashtag|scraped_order|          created_at|              id_str|                text|truncated|in_reply_to_screen_name|retweet_count|favorite_count|lang|    screen_name|    user_name|    user_description|user_verified|user_followers_count|            hashtags|symbols|og_tweet_by|og_tweet_truncated|row_id|neg|neu|pos|compound|row_id2|\n",
      "+---+---------------+-------------+--------------------+--------------------+--------------------+---------+-----------------------+-------------+--------------+----+---------------+-------------+--------------------+-------------+--------------------+--------------------+-------+-----------+------------------+------+---+---+---+--------+-------+\n",
      "| 26|       #vaccine|          0.0|Thu Mar 18 01:00:...|1.372352049828991...|More than 5,000 C...|     True|                   null|          1.0|           3.0|  en|CE_ChinaEconomy|China Economy|Latest business n...|         True|              2171.0|['Egypt', 'Sinoph...|     []|       null|              null|    26|0.0|1.0|0.0|     0.0|     26|\n",
      "+---+---------------+-------------+--------------------+--------------------+--------------------+---------+-----------------------+-------------+--------------+----+---------------+-------------+--------------------+-------------+--------------------+--------------------+-------+-----------+------------------+------+---+---+---+--------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Export to Directory of CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/sfs/qumulo/qhome/cam7cu/twitter_corpus_VADER already exists.;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7598bd391397>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'twitter_corpus_VADER'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1025\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/sfs/qumulo/qhome/cam7cu/twitter_corpus_VADER already exists.;"
     ]
    }
   ],
   "source": [
    "result.write.csv('twitter_corpus_VADER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czf twitter_corpus_VADER.tar.gz twitter_corpus_VADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Analyze Manual Labels vs VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.executor.memory\", '20g') \\\n",
    "        .config('spark.executor.cores', '8') \\\n",
    "        .config('spark.executor.instances', '2') \\\n",
    "        .config(\"spark.driver.memory\",'1g') \\\n",
    "        .appName(\"twitter_project\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.option(\"delimiter\", \"\\t\").option(\"header\", \"true\").csv(\"Manual_and_Preds.txt\")\n",
    "df.count() #count records in Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|label|\n",
      "+----------+-----+\n",
      "|       0.0|  1.0|\n",
      "+----------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "#using max of multiple columns, add single predicted valence column\n",
    "dfval = df.withColumn('prediction', f.col('PRED').cast('double'))\n",
    "                      \n",
    "#using max of multiple columns, add single manual valence column\n",
    "dfval = dfval.withColumn('label', f.col('TRUE').cast('double'))\n",
    "\n",
    "dfval = dfval.select(\"prediction\", \"label\")\n",
    "\n",
    "dfval.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER valence prediction accuracy = 0.4025\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"VADER valence prediction accuracy = \" + str(evaluator.evaluate(dfval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER valence prediction precision = 0.39185750636132316\n"
     ]
    }
   ],
   "source": [
    "# compute precision\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"precisionByLabel\")\n",
    "print(\"VADER valence prediction precision = \" + str(evaluator.evaluate(dfval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER valence prediction recall = 1.0\n"
     ]
    }
   ],
   "source": [
    "# compute recall\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"recallByLabel\")\n",
    "print(\"VADER valence prediction recall = \" + str(evaluator.evaluate(dfval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER valence prediction F1 score = 0.5630712979890311\n"
     ]
    }
   ],
   "source": [
    "# compute F1 score\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"fMeasureByLabel\")\n",
    "print(\"VADER valence prediction F1 score = \" + str(evaluator.evaluate(dfval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER valence prediction Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(prediction=0.0, -1.0=158, 0.0=154, 1.0=81),\n",
       " Row(prediction=-1.0, -1.0=2, 0.0=None, 1.0=None),\n",
       " Row(prediction=1.0, -1.0=None, 0.0=None, 1.0=5)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute confusion matrix\n",
    "print(\"VADER valence prediction Confusion Matrix\")\n",
    "dfval.groupBy(\"prediction\").pivot(\"label\").count().collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
